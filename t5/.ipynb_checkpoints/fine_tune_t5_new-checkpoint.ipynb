{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import os\n",
    "\n",
    "# Importing the T5 modules from huggingface/transformers\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "from rich.table import Column, Table\n",
    "from rich import box\n",
    "from rich.console import Console\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# define a rich console logger\n",
    "console=Console(record=True)\n",
    "\n",
    "def display_df(df):\n",
    "  \"\"\"display dataframe in ASCII format\"\"\"\n",
    "\n",
    "  console=Console()\n",
    "  table = Table(Column(\"source_text\", justify=\"center\" ), Column(\"target_text\", justify=\"center\"), title=\"Sample Data\",pad_edge=False, box=box.ASCII)\n",
    "\n",
    "  for i, row in enumerate(df.values.tolist()):\n",
    "    table.add_row(row[0], row[1])\n",
    "\n",
    "  console.print(table)\n",
    "\n",
    "training_logger = Table(Column(\"Epoch\", justify=\"center\" ), \n",
    "                        Column(\"Steps\", justify=\"center\"),\n",
    "                        Column(\"Loss\", justify=\"center\"), \n",
    "                        title=\"Training Status\",pad_edge=False, box=box.ASCII)\n",
    "\n",
    "class YourDataSetClass(Dataset):\n",
    "  \"\"\"\n",
    "  Creating a custom dataset for reading the dataset and \n",
    "  loading it into the dataloader to pass it to the neural network for finetuning the model\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, dataframe, tokenizer, source_len, target_len, source_text, target_text):\n",
    "    self.tokenizer = tokenizer\n",
    "    self.data = dataframe\n",
    "    self.source_len = source_len\n",
    "    self.summ_len = target_len\n",
    "    self.target_text = self.data[target_text]\n",
    "    self.source_text = self.data[source_text]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.target_text)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    source_text = str(self.source_text[index])\n",
    "    target_text = str(self.target_text[index])\n",
    "\n",
    "    #cleaning data so as to ensure data is in string type\n",
    "    source_text = ' '.join(source_text.split())\n",
    "    target_text = ' '.join(target_text.split())\n",
    "\n",
    "    source = self.tokenizer.batch_encode_plus([source_text], max_length= self.source_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "    target = self.tokenizer.batch_encode_plus([target_text], max_length= self.summ_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "\n",
    "    source_ids = source['input_ids'].squeeze()\n",
    "    source_mask = source['attention_mask'].squeeze()\n",
    "    target_ids = target['input_ids'].squeeze()\n",
    "    target_mask = target['attention_mask'].squeeze()\n",
    "\n",
    "    return {\n",
    "        'source_ids': source_ids.to(dtype=torch.long), \n",
    "        'source_mask': source_mask.to(dtype=torch.long), \n",
    "        'target_ids': target_ids.to(dtype=torch.long),\n",
    "        'target_ids_y': target_ids.to(dtype=torch.long)\n",
    "    }\n",
    "\n",
    "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
    "\n",
    "  \"\"\"\n",
    "  Function to be called for training with the parameters passed from main function\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  model.train()\n",
    "  for _,data in enumerate(loader, 0):\n",
    "    y = data['target_ids'].to(device, dtype = torch.long)\n",
    "    y_ids = y[:, :-1].contiguous()\n",
    "    lm_labels = y[:, 1:].clone().detach()\n",
    "    lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "    ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "    mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "    outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n",
    "    loss = outputs[0]\n",
    "\n",
    "    if _%10==0:\n",
    "      training_logger.add_row(str(epoch), str(_), str(loss))\n",
    "      console.print(training_logger)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "def validate(epoch, tokenizer, model, device, loader):\n",
    "\n",
    "  \"\"\"\n",
    "  Function to evaluate model for predictions\n",
    "\n",
    "  \"\"\"\n",
    "  model.eval()\n",
    "  predictions = []\n",
    "  actuals = []\n",
    "  with torch.no_grad():\n",
    "      for _, data in enumerate(loader, 0):\n",
    "          y = data['target_ids'].to(device, dtype = torch.long)\n",
    "          ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "          mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "          generated_ids = model.generate(\n",
    "              input_ids = ids,\n",
    "              attention_mask = mask, \n",
    "              max_length=150, \n",
    "              num_beams=20,\n",
    "              repetition_penalty=2.5, \n",
    "              length_penalty=1.0, \n",
    "              early_stopping=True,\n",
    "              num_return_sequences = 10\n",
    "              )\n",
    "          preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "          target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
    "          if _%10==0:\n",
    "              console.print(f'Completed {_}')\n",
    "\n",
    "          predictions.extend(preds)\n",
    "          actuals.extend(target)\n",
    "  return predictions, actuals\n",
    "\n",
    "def T5Trainer(train_dataset, val_dataset, source_text, target_text, model_params, output_dir=\"./outputs/\" ):\n",
    "  \n",
    "  \"\"\"\n",
    "  T5 trainer\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  # Set random seeds and deterministic pytorch for reproducibility\n",
    "  torch.manual_seed(model_params[\"SEED\"]) # pytorch random seed\n",
    "  np.random.seed(model_params[\"SEED\"]) # numpy random seed\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  # logging\n",
    "  console.log(f\"\"\"[Model]: Loading {model_params[\"MODEL\"]}...\\n\"\"\")\n",
    "\n",
    "  # tokenzier for encoding the text\n",
    "  tokenizer = T5Tokenizer.from_pretrained(model_params[\"MODEL\"])\n",
    "\n",
    "  # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n",
    "  # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "  model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
    "  model = model.to(device)\n",
    "  \n",
    "  # logging\n",
    "  console.log(f\"[Data]: Reading data...\\n\")\n",
    "\n",
    "  # # Importing the raw dataset\n",
    "  # dataframe = dataframe[[source_text,target_text]]\n",
    "  # display_df(dataframe.head(2))\n",
    "\n",
    "  \n",
    "  # Creation of Dataset and Dataloader\n",
    "  # Defining the train size. So 80% of the data will be used for training and the rest for validation. \n",
    "  # train_size = 0.8\n",
    "  # train_dataset=dataframe.sample(frac=train_size,random_state = model_params[\"SEED\"])\n",
    "  # val_dataset=dataframe.drop(train_dataset.index).reset_index(drop=True)\n",
    "  # train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "  # console.print(f\"FULL Dataset: {dataframe.shape}\")\n",
    "  console.print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
    "  console.print(f\"TEST Dataset: {val_dataset.shape}\\n\")\n",
    "\n",
    "\n",
    "  # Creating the Training and Validation dataset for further creation of Dataloader\n",
    "  training_set = YourDataSetClass(train_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
    "  val_set = YourDataSetClass(val_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
    "\n",
    "\n",
    "  # Defining the parameters for creation of dataloaders\n",
    "  train_params = {\n",
    "      'batch_size': model_params[\"TRAIN_BATCH_SIZE\"],\n",
    "      'shuffle': True,\n",
    "      'num_workers': 0\n",
    "      }\n",
    "\n",
    "\n",
    "  val_params = {\n",
    "      'batch_size': model_params[\"VALID_BATCH_SIZE\"],\n",
    "      'shuffle': False,\n",
    "      'num_workers': 0\n",
    "      }\n",
    "\n",
    "\n",
    "  # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "  training_loader = DataLoader(training_set, **train_params)\n",
    "  val_loader = DataLoader(val_set, **val_params)\n",
    "\n",
    "\n",
    "  # Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
    "  optimizer = torch.optim.Adam(params =  model.parameters(), lr=model_params[\"LEARNING_RATE\"])\n",
    "\n",
    "\n",
    "  # Training loop\n",
    "  console.log(f'[Initiating Fine Tuning]...\\n')\n",
    "\n",
    "  for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
    "      train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
    "      \n",
    "  console.log(f\"[Saving Model]...\\n\")\n",
    "  #Saving the model after training\n",
    "  path = os.path.join(output_dir, \"model_files\")\n",
    "  model.save_pretrained(path)\n",
    "  tokenizer.save_pretrained(path)\n",
    "\n",
    "\n",
    "  # evaluating test dataset\n",
    "  console.log(f\"[Initiating Validation]...\\n\")\n",
    "  for epoch in range(model_params[\"VAL_EPOCHS\"]):\n",
    "    predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
    "    final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
    "    final_df.to_csv(os.path.join(output_dir,'predictions.csv'))\n",
    "  \n",
    "  console.save_text(os.path.join(output_dir,'logs.txt'))\n",
    "  \n",
    "  console.log(f\"[Validation Completed.]\\n\")\n",
    "  console.print(f\"\"\"[Model] Model saved @ {os.path.join(output_dir, \"model_files\")}\\n\"\"\")\n",
    "  console.print(f\"\"\"[Validation] Generation on Validation data saved @ {os.path.join(output_dir,'predictions.csv')}\\n\"\"\")\n",
    "  console.print(f\"\"\"[Logs] Logs saved @ {os.path.join(output_dir,'logs.txt')}\\n\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import cuda\n",
    "cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk import tokenize\n",
    "def split_into_sentences(text):\n",
    "    tmp = tokenize.sent_tokenize(text)\n",
    "    return tmp[0], ' '.join(tmp[1:])\n",
    "\n",
    "def wiki_to_fine_tune_data(wiki_path):\n",
    "    df = pd.DataFrame()\n",
    "    f = open(wiki_path)\n",
    "    for line in f:\n",
    "        try:\n",
    "            line = json.loads(line)\n",
    "            first_sentence, completion = split_into_sentences(line['text'])\n",
    "            prompt = line['title'] + '\\n' + first_sentence\n",
    "            dic = {'prompt': prompt, 'completion': completion}\n",
    "            df = df.append(dic, ignore_index=True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    print(len(df))\n",
    "    df = df.sort_values(by='completion', key = lambda x: x.str.len(), ascending= False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data processed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[08:46:53] </span><span style=\"font-weight: bold\">[</span>Model<span style=\"font-weight: bold\">]</span>: Loading t5-base<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                      <a href=\"file:///state/partition1/job-18098167/ipykernel_2764637/1644203999.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1644203999.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///state/partition1/job-18098167/ipykernel_2764637/1644203999.py#154\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">154</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[08:46:53]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mModel\u001b[1m]\u001b[0m: Loading t5-base\u001b[33m...\u001b[0m                                      \u001b]8;id=42908;file:///state/partition1/job-18098167/ipykernel_2764637/1644203999.py\u001b\\\u001b[2m1644203999.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=9091;file:///state/partition1/job-18098167/ipykernel_2764637/1644203999.py#154\u001b\\\u001b[2m154\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m                                                                 \u001b[2m                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[08:47:01] </span><span style=\"font-weight: bold\">[</span>Data<span style=\"font-weight: bold\">]</span>: Reading data<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                          <a href=\"file:///state/partition1/job-18098167/ipykernel_2764637/1644203999.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1644203999.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///state/partition1/job-18098167/ipykernel_2764637/1644203999.py#165\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">165</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[08:47:01]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mData\u001b[1m]\u001b[0m: Reading data\u001b[33m...\u001b[0m                                          \u001b]8;id=55176;file:///state/partition1/job-18098167/ipykernel_2764637/1644203999.py\u001b\\\u001b[2m1644203999.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=521664;file:///state/partition1/job-18098167/ipykernel_2764637/1644203999.py#165\u001b\\\u001b[2m165\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m                                                                 \u001b[2m                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TRAIN Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2000</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TRAIN Dataset: \u001b[1m(\u001b[0m\u001b[1;36m2000\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TEST Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2000</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TEST Dataset: \u001b[1m(\u001b[0m\u001b[1;36m2000\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"font-weight: bold\">[</span>Initiating Fine Tuning<span style=\"font-weight: bold\">]</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                      <a href=\"file:///state/partition1/job-18098167/ipykernel_2764637/1644203999.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1644203999.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///state/partition1/job-18098167/ipykernel_2764637/1644203999.py#214\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">214</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mInitiating Fine Tuning\u001b[1m]\u001b[0m\u001b[33m...\u001b[0m                                      \u001b]8;id=273226;file:///state/partition1/job-18098167/ipykernel_2764637/1644203999.py\u001b\\\u001b[2m1644203999.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=432620;file:///state/partition1/job-18098167/ipykernel_2764637/1644203999.py#214\u001b\\\u001b[2m214\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m                                                                 \u001b[2m                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                               Training Status                                </span>\n",
       "+----------------------------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                             Loss                            </span>|\n",
       "|------+-------+-------------------------------------------------------------|\n",
       "|  0   |   0   | tensor(14.3284, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "+----------------------------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                               Training Status                                \u001b[0m\n",
       "+----------------------------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                            Loss                            \u001b[0m|\n",
       "|------+-------+-------------------------------------------------------------|\n",
       "|  0   |   0   | tensor(14.3284, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
       "+----------------------------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m test_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/scratch/yd2481/wiki/wiki_data/wiki_test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata processed\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m \u001b[43mT5Trainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompletion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/scratch/yd2481/wiki/t5/t5_trained_2sd_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mT5Trainer\u001b[0;34m(train_dataset, val_dataset, source_text, target_text, model_params, output_dir)\u001b[0m\n\u001b[1;32m    214\u001b[0m console\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[Initiating Fine Tuning]...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(model_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTRAIN_EPOCHS\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m--> 217\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m console\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Saving Model]...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m#Saving the model after training\u001b[39;00m\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch, tokenizer, model, device, loader, optimizer)\u001b[0m\n\u001b[1;32m    101\u001b[0m   console\u001b[38;5;241m.\u001b[39mprint(training_logger)\n\u001b[1;32m    103\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 104\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/ext3/miniconda3/envs/gpt/lib/python3.9/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ext3/miniconda3/envs/gpt/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_params={\n",
    "    \"MODEL\":\"t5-base\",             # model_type: t5-base/t5-large\n",
    "    \"TRAIN_BATCH_SIZE\":4,          # training batch size\n",
    "    \"VALID_BATCH_SIZE\":16,          # validation batch size\n",
    "    \"TRAIN_EPOCHS\":10,              # number of training epochs\n",
    "    \"VAL_EPOCHS\":1,                # number of validation epochs\n",
    "    \"LEARNING_RATE\":1e-4,          # learning rate\n",
    "    \"MAX_SOURCE_TEXT_LENGTH\":100,  # max length of source text\n",
    "    \"MAX_TARGET_TEXT_LENGTH\":512,   # max length of target text\n",
    "    \"SEED\": 42                     # set seed for reproducibility \n",
    "}\n",
    "\n",
    "# train_data_path = '/scratch/yd2481/wiki/wiki_data/wiki_fine_tune_2.json'\n",
    "# test_data_path = '/scratch/yd2481/wiki/wiki_data_final_3000.json'\n",
    "# train_data = wiki_to_fine_tune_data(train_data_path)[:10000]\n",
    "# test_data = wiki_to_fine_tune_data(test_data_path)[:2000]\n",
    "# train_data.to_csv('/scratch/yd2481/wiki/wiki_data/wiki_fine_tune.csv')\n",
    "# test_data.to_csv('/scratch/yd2481/wiki/wiki_data/wiki_test.csv')\n",
    "\n",
    "\n",
    "train_data = pd.read_csv('/scratch/yd2481/wiki/wiki_data/wiki_fine_tune.csv')\n",
    "test_data = pd.read_csv('/scratch/yd2481/wiki/wiki_data/wiki_test.csv')\n",
    "print('data processed')\n",
    "T5Trainer(train_dataset = train_data, val_dataset = test_data, source_text=\"prompt\", target_text=\"completion\", model_params=model_params, output_dir=\"/scratch/yd2481/wiki/t5/t5_trained_3rd_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/scratch/yd2481/wiki/t5/predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'on Romanticism were characterized by the Marxist-Leninist view of capitalism. Marx and Engels considered the criticisms of capitalism to be \"half lamentation, half lampoon; at other times it is a mockery of the present, striking the bourgeoisie to the very heart.\" Marx and Engels called the criticisms of capitalism \"feudal socialism\", which they described as \"the most virulent critique of capitalism in history\": \"The most virulent critique of capitalism was that of the past, but also of the present,\" and thus of the future. Marx and Engels rejected the notions of feudal socialism as an'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Generated Text'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pyotr Semyonovich Kogan, on the other hand, believed that the Romantics \"were, thanks to the strength of their criticism, able to discover many errors of the Enlightenment, which forced progressive writers to proceed more cautiously and not repeat the mistakes of the past.\" For A. Vishnevsky, \"the pathos of Romantic art lies in exposing the disharmony of the modern world, in an unaccountable striving for the integrity of human development and harmonious social relations. However, the struggle against the ugliness and philistinism of capitalist civilization takes on a reactionary-utopian character among the Romantics; illusory dreaminess and inability to sober objective study and depiction of reality are typical of Romantic art in general. These features of art show Romanticism\\'s departure from the tasks of realistic art, from the demand for artistic reflection of the real conditions of human historical activity. Due to this, the irrational and religious-mystical principle becomes an essential element of the Romantic art, and sometimes even the exclusive source of its poetic pathos (Novalis, Chateaubriand, Coleridge).Vladimir Lenin wrote of Romanticism: \"Unlike the enlighteners with their ardent belief in the progressiveness of this social development, with their merciless enmity, wholly and exclusively directed against the remnants of antiquity, the Romantic, falling into a reactionary illusion, commits his \"typical mistake\" - the conclusion from the contradictions of capitalism to the denial that [capitalism] contains a higher form of society. \"Hungarian philosopher György Lukács, in contrast to most of his Marxist contemporaries, claimed that Romanticism is a bourgeois and not feudal intellectual current, a movement in the crossroads of the great historical transformation of feudal big property into capitalist property. Georgi Plekhanov wrote of the Romantics: \"these people were accusers against the bourgeoisie, and eventually became apologists for capitalism\". Likewise, Dimitris Glinos was of the opinion that Romanticism represented \"a compromise between the middle class intelligentsia and the feudal'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Actual Text'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Marxist-Leninist views on Romanticism\\nKarl Marx and Friedrich Engels considered the Romantic-aristocratic critiques of capitalism as belonging to the current they called feudal socialism: \"half lamentation, half lampoon; half an echo of the past, half menace of the future; at times, by its bitter, witty and incisive criticism, striking the bourgeoisie to the very heart’s core; but always ludicrous in its effect, through total incapacity to comprehend the march of modern history.\"'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['prompt'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Barrett House (Poughkeepsie, New York)\\nBarrett House is a historic home located at Poughkeepsie, Dutchess County, New York, today home to Barrett Art Center.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['prompt'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This triple-landmark  (National, State, and municipal) Greek Revival brick townhouse was built in the early 1840s. The Barrett House reflects three phases of construction. The original building is a ca. 1842 three-story, three-bay by four-bay Greek Revival brick house with a side-gabled, stepped roof. A two-story, three-bay by two-bay, front-gabled brick addition was constructed to its rear ca. 1867. In the twentieth century, Barrett House achieved notoriety as the family home of Poughkeepsie-born WPA muralist Thomas Weeks Barrett. Jr. (1902-1947), who founded the Dutchess County Art Association (DCAA) in 1935 and lived there until his death in 1947. His artwork, family archive, and DCAA records remain in the house today. Thomas W. Barrett, Jr. graduated from the School of the Museum of Fine Arts, Boston in 1926, but his energies and artwork centered on the Hudson Valley. Barrett worked professionally as a designer, painter, printmaker, and as a muralist for the Treasury Relief Art Project (1936) and the Works Progress Administration (1937). As a Hudson Valley “American Scene” painter Barrett fashioned a modern iteration of the region's landscapes first immortalized a century earlier by the founders of the nation's first major art movement, the Hudson River School. Barrett turned his artistic attention to the urban landscapes of cities along the Hudson as symbols of a resilient and modern American character. Barrett organized the first art exhibition in Dutchess County at the Luckey Platt Department Store in 1934. Barrett founded the DCAA a year later. Barrett died in 1947 and his sister bequeathed the townhouse to the DCAA in 1974. The DCAA subsequently converted its first and second floor living spaces to four galleries, a community arts space, and offices, and operates under the name Barrett Art Center. Today, the third-floor studio Barrett designed in 1930, with a 7-foot by 9-foot north-facing sky-light, is an active studio used by an artist in residence. The DCAA collection includes artwork – much of it Barrett's – and archives including his family papers, film, photographs, manuscripts, memorabilia, and DCAA records, all of which remain in the house. The three-story, three bay brick building in the Greek Revival style. It is on a raised basement and features brownstone trim and a third story Eastlake style porch.It was added to the National Register of Historic Places in 1982.Early 19th century Poughkeepsie & the Construction of 55 Noxon Street\\nOne of the oldest communities along the Hudson River, was initially settled during the late seventeenth century. Though it grew slowly, it was well situated near the major transportation routes and was named the county seat in 1717. The village became a center of commerce and trade and by the nineteenth century its economy came to be dominated by industry and manufacturing. For about a decade starting in 1832, Poughkeepsie also became an important center of the regional whaling industry. During the late 1830s and early 1840s, Poughkeepsie experienced a real estate boom in response to the growth of these enterprises and the efforts of the local Improvement Party, a group of businessmen and politicians who boosted the City within the region. The area's role as a manufacturing center was spurred by the completion of the Hudson River railroad to Poughkeepsie in 1849.Virgil D. Bonesteel, an ambitious Yale graduate (Phi Beta Kappa, Class of 1827) and descendant of Red Hook's early settlers, was one of the new professionals attracted to the bright future of Poughkeepsie that was heavily promoted during the boom years of the 1830s. His first position was that of law student in the office of James Hooker, Dutchess County's Surrogate for 16 years and a dominant force in the awarding of political patronage jobs within the Democratic Party. Bonesteel quickly became an up-and-coming young leader in county and state Democratic Committee work and was appointed Clerk of the County Board of Supervisors. His rise in these political and legal circles may have been assisted by his marriage in 1840 to Sarah E. Todd of New Milford Connecticut, the niece of Poughkeepsie attorney and former Secretary of the Navy, Smith Thompson who was then serving as a Justice on the United States Supreme Court. Following the national financial panic of 1837, Poughkeepsie's booming real estate market ground to a near-halt. Bonesteel was able to take advantage of this, purchasing a lot for $1,325 on Noxon Street at a foreclosure auction in November 1841. The lot was made available by the financial collapse of shoe manufacturer Benjamin Bissell, one of the many who had ventured into real estate speculation during the ‘years of Poughkeepsie’s “Improvement Party” boom in the mid-1830s. Newly married, Virgil and Sarah Bonesteel began constructing their home soon after; construction most likely occurred in 1842 since Bonesteel is listed as residing at 55 Noxon St. in the first extant village directory of 1843. The builder is unknown. The completion of the elegant brick town home projected Bonesteel's success within the community. Well-to-do families of this period, such as the Bonesteels, expressed their taste within this restrained form of domestic architecture that symbolized a young nation's hopes for becoming the new embodiment of the purity, strength and equality of an idealized ancient Greece. With its wide frieze of wreathed attic windows, ornamental stoop railing, two story porch with fluted Doric columns, and recessed double front door with rectangular transom, the Bonesteels’ Greek Revival townhouse embodied the quiet elegance typical of this style. In 1844, Bonesteel was appointed to the position of Dutchess County Surrogate. But the rough and tumble of politics derailed Bonesteel's rise when his enemies in the opposing Whig party accused him of levying “bloodsucking” fees on defenseless widows and orphans forced to settle their estates in his court. Bonesteel's seemingly extravagant personal life also came under fire by his enemies in the Whig party newspaper who sarcastically observed, “We understand that our surrogate made an excursion the other day to New Milford WITH A COACH AND FOUR. Now it is clearly none of our business how he rides, but as he is now the leader of the party in the county, he must excuse us for feeling concerned about THE DEMOCRACY OF THE THING…”Whether it was a “coach and four” lifestyle, excessive real estate speculation or some other unknown factor, Bonesteel declared bankruptcy in 1848. In 1849, his extensive real estate holdings were sold to pay his debts. The Bonesteel home at 55 Noxon Street was described in foreclosure auction advertisements as a “large and spacious house, one of the most desirable in Poughkeepsie.”19th Century History of 55 Noxon Street\\nFrom 1849 to 1866, 55 Noxon St. was owned by Eliza Thompson, the widow of Supreme Court Judge Smith Thompson. Eliza Thompson was the daughter of Henry Livingston, Jr. and grew up on the Livingston estate we know today as Locust Grove. In 1836, she married the much older widower, Supreme Court Justice Smith Thompson and became the elegant young hostess of an elite political circle at her husband's riverfront estate “Rust Plaetz” (now part of Poughkeepsie Rural Cemetery). Eliza Thompson used 55 Noxon as an income-producing property, renting it to a series of well-to-do tenants. During the years she owned 55 Noxon Street, Eliza Thompson remarried and began a new life elsewhere. The rental of 55 Noxon Street may have been managed for her, possibly by Jennette Jewett who purchased the property in 1866 and two lots on Mill Street for $14,425. Jewett owned other properties in the city and was not new to the real estate world. In fact, she was the daughter of one of Poughkeepsie's first Main Street developers, the industrialist and inventor Gilbert Brewster. Jennette Jewett likely saw a good investment opportunity in 55 Noxon Street. Properties like it, which were being used as “high class boarding houses,” were in high demand in 1866. A year later, Jewett sold 55 Noxon for $7,000. By 1869, she had sold the property on Mill St. for $8,500 - netting a profit of $1,075 on the deal. The doubling of the house's value from $3,550 in 1849 to $7,000 in 1867 suggests that the large rear addition to the house was completed sometime between 1849 and 1867. In 1867, Jewett sold the property to retired Marlborough farmer Benjamin F. Townsend and his wife Lucy. Census records show that they operated 55 Noxon as a boarding house; assorted white-collar boarders occupied rooms in the house during the Townsend years at 55 Noxon Street. The number of boarders grew to as high as 14 by 1875 when Lucy Townsend (by then a widow) was described as “keeping a boarding house” in the federal census. At Lucy Townsend's death in 1879, the house was inherited by her wealthy nephew George W. Townsend. The house was sold by his heirs in 1882 to Captain James H. Wheeler and his wife Phebe. Wheeler's earliest years were said to have been spent at sea followed by work as a ship builder, hotel keeper and operator of a small boat on the Hudson. The nickname “Captain” followed him his entire life. During his years at 55 Noxon Street, from 1882 to 1887, Wheeler used the house as his family home and for his business as an awning and sail maker. After failing to sell the house at auction in 1887, the Wheelers sold it two years later to Miss Mary Elizabeth Weeks for $4,500. Within the decade after her purchase, the family likely added the rearmost addition to the house to update and expand its kitchen space. For the next three generations, the house would stay in the Weeks-Barrett family. The sister of prominent lawyer James H. Weeks, Mary purchased the house at a time when the family was in transition soon after her brother's death. Without the patriarch James Weeks, described in an 1881 newspaper article on real estate assessments as “the richest man in town,” the family began sinking into shabby gentility. The family's first decade in their new home at 55 Noxon was filled with loss. In 1892, Mary Elizabeth died of pneumonia. In 1893, Charles W. Barrett, the husband of James Weeks’ sister Eloise, also died. Perhaps most tragic was the loss of Eloise and Charles Barrett's son, the quiet and reserved young bank clerk Charles K. Barrett, who died of tuberculosis at age 27 in 1894. Finally, another of Weeks’ sisters, Emily Weeks Vary died in 1897. It must have seemed that the spell of sadness was at last broken in 1900 when Eloise and Charles Barrett's other son, Tom Barrett, married Miss Kate Stoutenburgh of Washington D.C. and Hyde Park. Unfortunately, a year later, Tom Barrett and his wife Kate were burying their infant son. However, Kate's arrival did start a new, more prosperous chapter in the family's history. Tom Barrett began a promising career at the Poughkeepsie National Bank, following in the footsteps of his deceased brother Charles and his uncle Isaac. At 55 Noxon Street, Tom and Kate Barrett settled into the quiet life of a small-town banker and his wife raising their two children, Thomas Jr. and Elizabeth, in a sheltered and loving environment with idyllic summers spent at the Putnam, Connecticut farm of Kate Barrett's father. Thomas Weeks Barrett, Jr. (1902-1947)\\nBorn in 1902, Tom Barrett enjoyed a sheltered and loving childhood at 55 Noxon Street. As an adult, Barrett gratefully noted that he had been born into a “protected sphere of family care which has not diminished.” The remarkably tender devotion and domesticity of the Barrett family is evident in the diaries, photos, letters and clippings the family lovingly saved as cherished objects which are now preserved in the Barrett Art Center's archive at 55 Noxon Street. In their public lives, the Barretts showed equally impressive loyalty to their community. Barrett's father, a banker for 54 years, was the trusted expert the community turned to when it needed a treasurer for major civic projects like saving the historic Glebe House from demolition, building St. Francis Hospital, or creating the Bowne Memorial Tuberculosis Sanitarium. Barrett's mother was engaged with the community as well, although in the traditionally acceptable ladylike activities of the D.A.R. and the women's auxiliary of Christ Church.Barrett began his career in the mid-1920s as a freelance commercial artist for New York City department stores and manufacturers - designing things like playing cards, book plates, wallpaper, and radio cabinets. Unable to make a living at this as the Depression deepened, Barrett returned home to 55 Noxon Street in 1929 where he lived for the rest of his life, working from the attic studio he created, which looked out over the backyards and rooftops of his “dear Poky.”\\nDuring the summers of 1928–1930, a fundamental shift began occurring in Barrett's life as an artist. While visiting friends in New Hampshire, Maine and Massachusetts, Barrett began experimenting with oil painting. His depictions of the mills, fishing sheds and wharves of New England caught the attention of art critics and launched Barrett in a new direction. His new success with oil painting brought exhibit opportunities in New York at the Anderson Gallery, Argent Gallery, Times Gallery, Fifteen Gallery, Academy of Allied Arts, and the Brooklyn Museum. Outside of New York City, Barrett's work appeared at the Pennsylvania Academy of Fine Arts, the Philadelphia Print Club, the Albany Institute of History and Art, the Connecticut Academy of Fine Arts, the Palm Beach Art Club, the Wood Cut Society of Kansas City and the Wichita Art Association. In the Hudson Valley, Barrett held one man shows at Bard College and the Hudson River Museum in Yonkers. Like other Depression era artists, Barrett also found work as a WPA muralist in 1936 and 1937 and became deeply committed to the art of social realism and the goal of introducing art into everyday life through murals and free public exhibitions.Throughout the 1930s, Barrett continued to more fully explore localism, an art movement that focused on looking for the universal in the particular. He obsessively painted Poughkeepsie, his own home town, in all its desolate and gritty beauty. In 1941, Barrett achieved one of his proudest moments when art critic and collector Duncan Phillips purchased his painting “Downtown Poughkeepsie” for the Phillips Memorial Gallery in Washington D.C.Tom Barrett Jr.’s focus extended from painting Poughkeepsie to growing its arts community and improving the local quality of life. He pursued a particularly bold agenda for community betterment, advocating for a municipal art gallery, a civic center, and a reimagined waterfront long before others in his community realized the importance of these key elements in urban design. From 55 Noxon Street, Tom Barrett also organized the county’s first art show in 1934. The show, which was held on the top floor of the Luckey-Platt Department store, included the work of approximately 50 local artists. A few months before his death in 1984, fellow artist Vince Walker remembered how it all began:\\nA group of us used to meet in Barrett’s studio to do some sketching and drawing. Tom and I thought it would be a good idea to get an exhibition together so we drove around the county and talked to some artists. Tom contacted Eleanor Roosevelt and some others to sponsor the exhibit. There really was no art community in Poughkeepsie at the time. Exclusive of exhibits at Vassar College, this was the first art exhibition in the community.Over 2,000 people visited this first ever regional art exhibit in the auditorium on the top floor of the Luckey-Platt Department Store. The press pronounced it “exhilarating” and an “eye opener of startling dimensions.” The community seemed stunned not just by the talent they saw revealed but also by how oblivious they had been to its existence. The following year, Tom Barrett and Vince Walker formalized this initial success by founding the Dutchess County Art Association. Tom Barrett served as its first president.Never content with leaving art inside a gallery, Barrett also became county chairman of the national program “American Art Week” which promoted the placement of art in the windows of stores, banks, theaters, and hotels in cities and towns across the country during the first week of each November. Barrett's sister Elizabeth, known to her family as Bet, supported her brother's efforts to grow and sustain the local arts community. A long-time employee of the Poughkeepsie Bank and Trust Company, she helped the arts community with her financial skills. For example, she served as treasurer of the “Victory Calendar” project of 1943. This fundraiser for local war work agencies captured the attention of President Roosevelt, who invited the Victory Calendar artists to hold an exhibit of their work at the FDR Library with a special opening ceremony hosted by the first lady.Barrett's most cherished dream, however, remained the creation of a dedicated space for art exhibits and classes. During much of Barrett's life time, artists of the region had to rely on leased or donated exhibit space in various venues such as the Luckey-Platt Department Store, the Hotel Campbell, Three Arts Bookstore, the Elverhoj Art Shop, the IBM Country Club, and the county fair. Art classes had to be arranged in the evenings at the public school. Barrett's dedication to finding a better and more permanent alternative to these makeshift arrangements even extended to planning out how his own home at 55 Noxon Street could be redesigned as an art gallery with small private quarters at the back of the house for himself, his sister Betty and his parents – if only an “angel” were to step forward with the funding.However, the haunting quality of Barrett's depictions of Poughkeepsie took its toll on Barrett's emotional and physical health. Privately, he mused that the intensity of producing such honest and original art would probably result in his early death. Barrett sought relief from the intensity of oil painting by also becoming a self-taught woodcut artist and printmaker. Barrett scholars Karal Ann Marling and Helen Harrison have noted that in woodcuts and printmaking, Barrett was able to return to his roots in the decorative arts. In woodcut work, Barrett could indulge his special talent for advanced linear thinking which, back in his student days, had convinced him to major in decorative arts and design rather than painting. Among the results of Barrett's woodcut work are some particularly beautiful architectural images that attracted many, including FDR. For his private collection, President Roosevelt purchased a Barrett woodcut of St. James Church in Hyde Park. Barrett's brilliant woodcuts and haunting oil paintings remain important examples of Depression era regionalism in Hudson Valley art history. Despite his extraordinary talent and determination, Tom Barrett's life was filled with challenges too great to surmount: alcoholism, a deep sense of guilt for failing to earn even a modest income from his art, and tormented feelings about a hometown he found both dishearteningly provincial and desolately beautiful. The international horrors of World War II, in particular the use of the atomic bomb, seemed to shake Barrett deeply. At home, the death of Barrett's father in 1944 and the new need to share the family's beloved home with income-producing tenants were difficult burdens to bear. In 1947, Tom Barrett died prematurely as he had predicted. He was 45 years old. In responding to a condolence letter, Barrett's mother wrote simply about her son: “While his life was short, it was full; but he was unhappy about the many sad things in the world that are happening to people.” His home at 55 Noxon Street was the deeply loved touchstone of Barrett's world, serving as both an art studio and a refuge for a gifted but vulnerable soul who found there, with the help of a loving family, the strength to continue believing in himself and the importance of art in the modern world\\nIn 1974, 27 years after her brother's death, Barrett's beloved sister Betty fulfilled her brother's dream by bequeathing 55 Noxon to the Dutchess County Art Association as its permanent home – now known as the Barrett Art Center. After taking ownership of Barrett House, the DCAA replaced the failing roof and equipped former domestic spaces on the first floor as galleries, maintaining interior woodwork, fireplaces, and surrounds. On the second floor, two spaces used as bedrooms by the Barrett family were combined to create an art classroom space. Back bedrooms were converted to a library, collections, storage and a gallery space. Barrett's third floor studio was transformed into a printmaking studio with a fire escape. Barrett House maintained this layout until 2017, when the print studio equipment on the third floor was moved to an accessible community space (the Poughkeepsie Underwear Factory). Since then the space has returned to its intended use as an artists’ studio.\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['completion'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "gpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
